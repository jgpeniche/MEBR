---
title: "M茅todos Estad铆sticos con R"
subtitle: "CODD"
author: "Gibr谩n Peniche"
date: "(versi贸n 0.0.1) - 2020/06/25"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["xaringan-themer.css", "style.css"]
    nature:
      highlightStyle: github
      highlightLines: TRUE
      countIncrementalSlides: TRUE
      self_contained: TRUE
      ratio: '16:9'
    seal: false
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(eval = TRUE)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.align = 'center')
library(tidyverse)
library(gt)
library(patchwork)
library(tidybayes)
library(latex2exp)

```

class: title-slide, middle

.pull-left[ 
# M茅todos Estad铆sticos Bayesianos con R
## Contraste de Hip贸tesis (Compuesta vs. Compuesta)
### Gibr谩n Peniche
### v. 0.0.1
### 2020-10-29
####  <i class="fab fa-github"></i> [jgpeniche](https://github.com/jgpeniche)
####  <i class="fab fa-twitter"></i> [PenicheGibran](https://twitter.com/PenicheGibran)
####  <i class="fab fa-google"></i> jgpeniche@gmail.com

]

.pull-right[

![](figs/logo_g.png)

]

---

# La sesi贸n pasada...

--

- Estudiamos el contexto hist贸rico y las dos propuestas para el contraste de hip贸tesis que se plantearon en la escuela de la Estad铆stica Matem谩tica

--

- Recordamos la soluci贸n al problema de contraste de hip贸tesis propuesto por Neyman y Pearson

--

- Resolvimos el problema de hip贸tesis simple contra simple desde el punto de vista Bayesiano

---

# Agenda

  1. Resolver el problema de contraste de hip贸tesis compuesta contra compuesta
  
---

class: inverse, center, middle

# 1

--

# Contraste de Hip贸tesis (Compuesta contra Compuesta)

---

# Contraste de Hip贸tesis (Compuesta contra Compuesta)

--

Consideremos un $\underline{X}_{(n)}$ con $X$ con f.d.p.g $f(x, \theta)$ con $\theta$ $\epsilon$ $\Theta$ $\subseteq$ $\mathbb{R}^k$.

--

Nos preguntamos por el juego de hip贸tesis $$H_0: \theta \quad \epsilon \quad A \quad \subseteq \quad \Theta \quad vs \quad H_1: \theta \quad \epsilon \quad A^c \quad \subseteq \quad \Theta$$
--

Nos dimos cuenta que el problema de Contraste de Hip贸tesis es equivalente a un problema de decisi贸n $$(\prec, \mathbb{E}, \mathbb{C}, \mathfrak{C})$$

--

Equivalente a $\mathbb{D} = \{d_i, d_j \}$ tal que $d_k = H_k: f(x,\theta) =f(x, \theta \epsilon A^k)$

--

Adem谩s si definimos una funci贸n de p茅rdida $\mathcal{L}(c(d_k, \theta)) = l_0(\theta)$

---

# Contraste de Hip贸tesis (Compuesta contra Compuesta)

--

$$\mathbb{E}[\mathfrak{L}_i] < \mathbb{E}[\mathfrak{L}_k]$$

--

En el caso **continuo** forzozamente la p茅rdida esperada *a posteriori* queda definida como $$\mathbb{E}[\mathcal{L}(d_k,\theta)] = \int_{\Theta} l_k(\theta) P(\theta | \underline{X}_{(n)}) \partial \theta$$

--

En consecuencia la regla decisi贸n del cociente de p茅rdidas esperadas est谩 dada por $$\Lambda = \frac{\int_{\Theta} l_i(\theta) P(\theta | \underline{X}_{(n)}) \partial \theta}{\int_{\Theta} l_j(\theta) P(\theta | \underline{X}_{(n)}) \partial \theta} < 1$$

--

A diferencia del cociente de verosimilitudes genralizado $\Lambda = \frac{\sup_{H_i} \mathbb{L}[\theta_i|\underline{X}_{(n)}]}{sup_{H_i \cup H_j} \mathbb{L}[\theta_{MV} | \underline{X}_{(n)}]}$, las verosimilitudes bayesianas est谩n penalizadas por $l_i(\theta)$ con respecto de la medida de probabilidad $P(\theta)$ y no con respecto de los supremos.

---

# Contraste de Hip贸tesis (Compuesta contra Compuesta)

Si adem谩s. al igual que en casi simple contra simple, simplificamos la funci贸n p茅rdida tal que asigne p茅rdida 0 al acierto y una constante para el error:


$$l_k(\theta) = \left\{ \begin{array}{rcl} 0 & \mbox{si} & \theta \epsilon A^k \\ l_k & \mbox{si} & o.c \end{array}\right.$$

--

El cociente se simplifica y est谩 dado por:

$$\Lambda = \frac{l_j \int_{A}  P(\theta | \underline{X}_{(n)}) \partial \theta}{l_i \int_{A^c} P(\theta | \underline{X}_{(n)}) \partial \theta} = \frac{l_j P(\theta \epsilon A)\int_{A}  P(\theta | \underline{X}_{(n)})P(\theta | \theta \epsilon A) \partial \theta}{l_i P(\theta \epsilon A^c) \int_{A^c} P(\theta | \underline{X}_{(n)})P(\theta | \theta \epsilon A^c) \partial \theta} = \frac{\mathbb{E}[L | H_0]}{  \mathbb{E}[L | H_1]} < 1$$
--

A esta contidad $\Lambda = BF$ tambipen se le conoce c贸mo **Factor de Bayes** y tiene la desventaja de que la posterior puede no tener forma an谩litica cerrada y requiere de *integraci贸n num茅rica* pero se puede generalizar para realizar el tipo de contraste $\mathcal{F}_i$ vs $\mathcal{F}_j$ dos familias param茅tricas distintas. 

---

# Contraste de Hip贸tesis (Compuesta contra Compuesta)

A pesar de la discusi贸n que acabamos de desarrollar, en sesiones pasadas concluimos que pod铆amos realizar pruebas de hip贸tesis a partir de nuestra *distribuci贸n posterior* y la distribuci贸n *posterior predictiva*. 驴Podemos hacer esa conjetura a partir de la misma discusi贸n?

--

Ocurre que *a priori* $$\frac{l_j \int_{A} P(\theta) \partial \theta}{l_j \int_{A^c} P(\theta) \partial \theta} = \frac{l_i P(A)}{l_j P(A^c)}<1$$

--

Esto es equivalente a la regla *a posteriori* $\frac{l_i P(A | \underline{X}_{(n)})}{l_j P(A^c | \underline{X}_{(n)})} < 1$

--

Noten que $P(A | \underline{X}_{(n)}) = \int_{A} P(\theta | \underline{X}_{(n)}) \partial \theta = P(\theta \epsilon A | \underline{X}_{(n)})$ que es de hecho la probabilidad posterior de $\theta$ en una regi贸n. El caso que discutimos en clase fue el caso particular donde los errores producen la misma p茅rdida.

---

# Contraste de Hip贸tesis (Compuesta contra Compuesta)

Desde la perpspectiva frecuentista la expresi贸n $P(\underline{X}_{(n)} | A)$ no tiene sentido en tanto que no determina un modelo p ara los datos que permita el c谩lculo de probabilidades sino una familia La relaci贸n entre la muestra ヰy los valores de  se describe a trav茅s de un modelo de la forma $P(\underline{X}_{(n)} | \theta)$

--

Esta expresi贸n equiere el valor puntual de y aqu铆 cobra especial relevancia la noci贸n de que, para el an谩lisis Bayesiano, el modelo de probabilidad no s贸lo incluye la componente que describe la variabilidad de los datos, sino tambi茅n al que describe la incertidumbre sobre el par谩metro fijo pero desconocido

---

# 驴Qu茅 sigue?

--

1. Estad铆stica Bayesiana Computacional

--

2. MCMC; Monte Carlo Markov Chain y todo su aparato te贸rico

